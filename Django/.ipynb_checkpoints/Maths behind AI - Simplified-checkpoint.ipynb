{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qaDsK9atlIfo"
   },
   "source": [
    "<img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"360\" />\n",
    "\n",
    "# Maths behind AI - Simplified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCYGAYXflIfq"
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Data Types Used in AI](#section1)<br>\n",
    "  - 1.1 [Scalars (0D tensors)](#section101)<br>\n",
    "  - 1.2 [Vectors (1D tensors)](#section102)<br>\n",
    "  - 1.3 [Matrices (2D tensors)](#section103)<br>\n",
    "  - 1.4 [3D tensors and Higher-Dimensional Tensors](#section104)<br>\n",
    "  - 1.5 [Key attributes of Tensors](#section105)<br>\n",
    "  - 1.6 [Manipulating tensors in Numpy](#section106)<br>\n",
    "  - 1.7 [The Notion of Data Batches](#section107)<br>\n",
    "  - 1.8 [Real-world examples of data tensors](#section108)<br><br>\n",
    "2. [Vector Data](#section2)<br><br>\n",
    "3. [Tensor Operations in a Nutshell](#section3)<br>\n",
    "  - 3.1 [Element-wise Operations](#section301)<br>\n",
    "  - 3.2 [Broadcasting](#section302)<br>\n",
    "  - 3.3 [Tensor Dot](#section303)<br>\n",
    "  - 3.4 [Tensor Reshaping](#section304)<br>\n",
    "  - 3.5 [A Geometric Interpretation of Deep Learning](#section305)<br><br>\n",
    "4. [Basic Maths for Gradient Descent](#section4)<br>\n",
    "  - 4.1 [What’s a derivative?](#section401)<br>\n",
    "  - 4.2 [Derivative of a Tensor Operation: the Gradient](#section402)<br>\n",
    "  - 4.3 [Stochastic Gradient Descent](#section403)<br>\n",
    "  - 4.4 [Chaining Derivatives: the Backpropagation Algorithm](#section404)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0X_O-oBlIfr"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/title.jpg\" width=\"800\" height=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7i_ujMY9lIft"
   },
   "source": [
    "<a id=section1></a>\n",
    "## 1. Data Types Used in AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oHp9xSL5lIfv"
   },
   "source": [
    "- **Tensors**: **Data stored in multidimensional Numpy arrays** are called **tensors**. \n",
    "\n",
    "\n",
    "- In general, all **current machine-learning systems use tensors** as their **basic data structure**. \n",
    "\n",
    "\n",
    "- **Tensors** are **fundamental** to the **field**—so fundamental that **Google’s TensorFlow** was **named after them**. \n",
    "\n",
    "\n",
    "- So what’s a tensor? At its core, **a tensor is a container for data—almost always numerical data**. So, it’s a **container for numbers**. \n",
    "\n",
    "\n",
    "- You may be already familiar with **matrices, which are 2D tensors**: **tensors** are a **generalization of matrices** to an **arbitrary number of dimensions** (note that in the **context of tensors**, a **dimension** is often **called an axis**).\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/tensor.png\" width=\"600\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rB6BbBz4lIfw"
   },
   "source": [
    "<a id=section101></a>\n",
    "### 1.1 Scalars (0D tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4_tvyFqlIfz"
   },
   "source": [
    "- A **tensor** that **contains** only **one number** is called a **scala**r (or **scalar tensor**, or **0-dimensional tensor**, or **0D tensor**). \n",
    "\n",
    "\n",
    "- In **Numpy**, a **float32 or float64 number** is a **scalar tensor** (or **scalar array**). \n",
    "\n",
    "\n",
    "- You can **display** the **number of axes** of a **Numpy tenso**r via the **ndim attribute**; a **scalar tensor has 0 axes (ndim == 0)**. \n",
    "\n",
    "\n",
    "- The **number of axes** of a **tensor** is also **called its rank**. \n",
    "\n",
    "\n",
    "- Here’s a **Numpy scalar**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vkF1GqUolIf1"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBh1If62lIf7",
    "outputId": "52665e3b-58c7-4e72-c60f-5a398dc0a988"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(12)"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8D6phpMDlIgB",
    "outputId": "19b244d9-4869-4882-eda8-440e5bd5f57c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "afTijKK6lIgH"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CN0ZgdWSlIgJ"
   },
   "source": [
    "<a id=section102></a>\n",
    "### 1.2 Vectors (1D tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "diKxb08slIgL"
   },
   "source": [
    "- An **array of numbers** is called a **vector**, or **1D tensor**. \n",
    "\n",
    "\n",
    "- A **1D tensor** is said to **have exactly one axis**. \n",
    "\n",
    "\n",
    "- Following is a **Numpy vector**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mfo9MM8NlIgM",
    "outputId": "a30e4caf-9f52-4928-b9df-9c56855c10ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12,  3,  6, 14, 30])"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([12, 3, 6, 14, 30])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QDRi0VeRlIgR",
    "outputId": "3a6b0240-391a-4d96-a492-32eb7b83405f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RzBafSMPlIgW"
   },
   "source": [
    "- This **vector** has **five entries** and so is called a **5-dimensional vector**. \n",
    "\n",
    "\n",
    "- **Don’t confuse** a **5D vector** with a **5D tensor**! A **5D vector** has only **one axis** and has **five dimensions along its axis**, whereas a **5D tensor** has **five axes** (and may have **any number of dimensions along each axis**). \n",
    "\n",
    "\n",
    "- **Dimensionality** can **denote either** the **number of entries along a specific axis** (as **in** the **case of our 5D vector**) or the **number of axes in a tensor** (such as a **5D tensor**), which can be confusing at times. \n",
    "\n",
    "\n",
    "- In the latter case, it’s **technically more correct** to talk about a **tensor of rank 5** (the **rank of a tensor** being the **number of axes**), but the ambiguous notation 5D tensor is common regardless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6IlCPUN3lIgY"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MABc7UnBlIgZ"
   },
   "source": [
    "<a id=section103></a>\n",
    "### 1.3 Matrices (2D tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUKs92KPlIga"
   },
   "source": [
    "- An **array of vectors** is a **matrix**, or **2D tensor**. \n",
    "\n",
    "\n",
    "- A **matrix has two axes** (often referred to **rows and columns**). You can **visually interpret** a **matrix** as a **rectangular grid of numbers**.\n",
    "\n",
    "\n",
    "- This is a **Numpy matrix**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DMssRi3ClIgd"
   },
   "outputs": [],
   "source": [
    "x = np.array([[5, 78, 2, 34, 0],\n",
    "              [6, 79, 3, 35, 1],\n",
    "              [7, 80, 4, 36, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XbzTqzn1lIgj",
    "outputId": "70d85e32-d60b-4152-93aa-f1bfb8c503fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JHihXLhalIgn"
   },
   "source": [
    "- The **entries from** the **first axis** are called the **rows**, and the **entries from** the **second axis** are called the **columns**. \n",
    "\n",
    "\n",
    "- In the example, **[5, 78, 2, 34, 0]** is the **first row** of **x**, and **[5, 6, 7]** is the **first column**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ydwTuLkdlIgp"
   },
   "source": [
    "<a id=section104></a>\n",
    "### 1.4 3D tensors and Higher-Dimensional Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5NvzgWJGlIgr"
   },
   "source": [
    "- If you **pack matrices** in a **new array**, you **obtain** a **3D tensor**, which you can **visually interpret** as a **cube of numbers**.\n",
    "\n",
    "\n",
    "- Following is a **Numpy 3D tensor**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9mZRngRglIgt"
   },
   "outputs": [],
   "source": [
    "x = np.array([[[5, 78, 2, 34, 0],\n",
    "               [6, 79, 3, 35, 1],\n",
    "               [7, 80, 4, 36, 2]],\n",
    "              [[15, 78, 32, 34, 80],\n",
    "               [16, 79, 33, 35, 81],\n",
    "               [17, 80, 34, 36, 82]],\n",
    "              [[45, 78, 52, 34, 60],\n",
    "               [46, 79, 53, 35, 71],\n",
    "               [47, 80, 54, 36, 72]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s013JW8vlIgy",
    "outputId": "4fb7271f-b65e-4a14-a0a4-26c6abdde497"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FsaEJPG0lIg7"
   },
   "source": [
    "- By **packing 3D tensors** in an **array**, you can **create a 4D tensor**, and so on. \n",
    "\n",
    "\n",
    "- In **deep learning**, you’ll generally **manipulate tensors** that are **0D to 4D**, although you may go up to **5D** if you **process video data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m1ZtAXPdlIg9"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnlOynXblIg_"
   },
   "source": [
    "<a id=section105></a>\n",
    "### 1.5 Key attributes of Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gt0HVgKUlIhA"
   },
   "source": [
    "A **tensor** is defined by **three key attributes**:\n",
    "\n",
    "- **Number of axes (rank)** - For instance, a **3D tensor has three axes**, and a **matrix has two axes**. This is also **called the tensor’s ndim in** Python libraries such as **Numpy**.\n",
    "\n",
    "\n",
    "- **Shape** - This is a **tuple of integers** that **describes** how many **dimensions** the **tensor has along each axis**. For instance, the previous **matrix example has shape (3, 5)**, and the **3D tensor example has shape (3, 3, 5)**. A **vector** has a **shape** with a **single element**, such as **(5,)**, whereas a **scalar** has an **empty shape, ()**.\n",
    "\n",
    "\n",
    "- **Data type** (usually called **dtype** in Python libraries) - This is the **type of the data contained in the tensor**; for instance, a **tensor’s type** could be **float32, uint8, float64**, and so on. \n",
    "\n",
    "  - On **rare occasions**, you may see a **char tensor**. \n",
    "  \n",
    "  - Note that **string tensors don’t exist in Numpy** (or in most other libraries), because **tensors live in preallocated, contiguous memory segments**: and **strings**, being **variable length**, would **preclude** the use of **this implementation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JoQ7gbMBlIhC"
   },
   "source": [
    "- To make this more concrete, let’s look back at the data we processed in the above examples.\n",
    "\n",
    "\n",
    "- We will **display** the **number of axes** of the **tensor x**, the **ndim attribute**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mUw4HMxolIhE",
    "outputId": "505792f6-bc35-4d19-9918-b8667d4eca1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(x.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uHgrykpwlIhY"
   },
   "source": [
    "- Here’s its **shape**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KLcnkoAulIha",
    "outputId": "6931c1a5-d142-445c-cad3-476d3ef28f56",
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 5)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ekSspyWblIhg"
   },
   "source": [
    "- And this is its **data type**, the **dtype attribute**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVpKjHXglIh7",
    "outputId": "10d94aeb-0ca4-4425-945e-39fd155767ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n"
     ]
    }
   ],
   "source": [
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mv8o915elIiP"
   },
   "source": [
    "- So what we have here is a **3D tensor** of **32-bit integers**. More precisely, it’s an **array of 3 matrices of 3 × 5 integers**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PHcfTQFlIiW"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVqvhFSelIiX"
   },
   "source": [
    "<a id=section106></a>\n",
    "### 1.6 Manipulating tensors in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yuM4xFMMlIib"
   },
   "source": [
    "- **Selecting specific elements** in a **tensor** is called **tensor slicing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8k7dOonFlIie",
    "outputId": "c116f7aa-86b9-4aa5-c6d5-41c70a32a3ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[45, 78, 52, 34, 60],\n",
       "       [46, 79, 53, 35, 71],\n",
       "       [47, 80, 54, 36, 72]])"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCvkHYP9lIjq"
   },
   "source": [
    "- In the previous example, we **selected** a **specific digit alongside** the **first axis** using the **syntax x[i]**. \n",
    "\n",
    "\n",
    "- Let’s look at the **tensor-slicing operations** you can do on **Numpy arrays**. The following example **selects matrices #0 to #2** (#2 isn’t included) and puts them in an **array of shape (2, 3, 5)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o5rLWUSRlIjv",
    "outputId": "6c914fa3-b451-46da-f864-b58e462e6b2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 5)\n"
     ]
    }
   ],
   "source": [
    "my_slice = x[0:2]\n",
    "print(my_slice.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrM_DDrUlIj3"
   },
   "source": [
    "- It’s **equivalent** to this **more detailed notation**, which **specifies** a **start index** and **stop index for** the **slice along each tensor axis**. \n",
    "\n",
    "\n",
    "- Note that **:** is **equivalent** to **selecting the entire axis**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bQwr2aG_lIku",
    "outputId": "bd6819f8-956c-45aa-821d-04aa4669ca00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 5)"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_slice = x[0:2, :, :]\n",
    "my_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Azb2gJU4lIk5",
    "outputId": "dc739955-14e1-4921-e32a-248ec21b6f25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 4)"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_slice = x[0:2, 0:1, 0:4]\n",
    "my_slice.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9n_hGGbFlIlH"
   },
   "source": [
    "- In general, you may **select between any two indices along each tensor axis**. \n",
    "\n",
    "\n",
    "- For instance, in order **to select 2 × 2 matrix** in the **bottom right corner** of all the **parent matrices**, you do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k4BGW330lIlI",
    "outputId": "c3900459-f5eb-485b-867c-e1e1b40599cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5, 78,  2, 34,  0],\n",
       "        [ 6, 79,  3, 35,  1],\n",
       "        [ 7, 80,  4, 36,  2]],\n",
       "\n",
       "       [[15, 78, 32, 34, 80],\n",
       "        [16, 79, 33, 35, 81],\n",
       "        [17, 80, 34, 36, 82]],\n",
       "\n",
       "       [[45, 78, 52, 34, 60],\n",
       "        [46, 79, 53, 35, 71],\n",
       "        [47, 80, 54, 36, 72]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FwpefLZ7lIlO",
    "outputId": "6a4cf15c-f334-4252-a932-5d91dbf699b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[35,  1],\n",
       "        [36,  2]],\n",
       "\n",
       "       [[35, 81],\n",
       "        [36, 82]],\n",
       "\n",
       "       [[35, 71],\n",
       "        [36, 72]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_slice = x[:, 1:, 3:]\n",
    "my_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3dt5GTEhlIlU"
   },
   "source": [
    "- It’s also **possible** to **use negative indices**. \n",
    "\n",
    "\n",
    "- Much **like negative indices in Python lists**, they **indicate a position relative to the end of the current axis**. \n",
    "\n",
    "\n",
    "- In order to **select** the **central row** of **each parent matrix**, you do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VKrmntxNlIlV",
    "outputId": "745be2b6-1fc3-463b-9ed7-9dfd6e3dcd74",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 6, 79,  3, 35,  1]],\n",
       "\n",
       "       [[16, 79, 33, 35, 81]],\n",
       "\n",
       "       [[46, 79, 53, 35, 71]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_slice = x[:, 1:-1, :]\n",
    "my_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFPBmHERlIlZ"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpfFDMxulIla"
   },
   "source": [
    "<a id=section107></a>\n",
    "### 1.7 The Notion of Data Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZB2qKuNJlIld"
   },
   "source": [
    "- In general, the **first axis (axis 0, because indexing starts at 0)** in all **data tensors** you’ll come across **in deep learning** will be the **samples axis** (sometimes called the **samples dimension**). \n",
    "\n",
    "\n",
    "- In addition, ****deep-learning models don’t process**** an ****entire dataset at once****; rather, ****they break the data into small batches****. \n",
    "\n",
    "\n",
    "\n",
    "- Concretely, here’s **one batch of our x**, with **batch size of 1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQEXAAY1lIlf",
    "outputId": "8a6b7a4a-e88c-4799-b18f-671bd2365c89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5, 78,  2, 34,  0],\n",
       "        [ 6, 79,  3, 35,  1],\n",
       "        [ 7, 80,  4, 36,  2]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = x[:1]\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UcxzPNDUlIlm"
   },
   "source": [
    "- And here’s the **next batch**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EOOICWpSlIlo",
    "outputId": "661bd169-57bb-49fb-a565-9d96dbba48f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[15, 78, 32, 34, 80],\n",
       "        [16, 79, 33, 35, 81],\n",
       "        [17, 80, 34, 36, 82]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = x[1:2]\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AUa868oblIlv"
   },
   "source": [
    "- The **batch size** is the **difference between** the **two values** inputed in the **slice above (batch_size = 2 - 1 = 1)**.\n",
    "\n",
    "\n",
    "- And the **nth batch**: **batch = x[1 * n:1 * (n + 1)]**\n",
    "\n",
    "\n",
    "- Here the **value before :** is the **multiple of n and batch_size**; and **value after :** is the **multiple of n+1 and batch_size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "29rLuduJlIlw",
    "outputId": "c98a46d2-b18e-49e3-a6c8-3c423156140f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[45, 78, 52, 34, 60],\n",
       "        [46, 79, 53, 35, 71],\n",
       "        [47, 80, 54, 36, 72]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if n = 2\n",
    "n = 2\n",
    "batch = x[(1 * n):(1 * (n + 1))]\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fGGEQuOrlIl0"
   },
   "source": [
    "- When considering such a **batch tensor**, the **first axis (axis 0)** is **called** the **batch axis** or **batch dimension**. \n",
    "\n",
    "\n",
    "- This is a **term** you’ll **frequently encounter** when **using Keras** and **other deep-learning libraries**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNRM9eeVlIl1"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qokhukB9lIl2"
   },
   "source": [
    "<a id=section108></a>\n",
    "### 1.8 Real-world examples of data tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GxRpdAWElIl3"
   },
   "source": [
    "- The **data** you’ll manipulate **will** almost always **fall into one** of the following **categories**:\n",
    "<br><br>\n",
    "  - **Vector data** - **2D tensors** of **shape (samples, features)**.\n",
    "  <br><br>\n",
    "  - **Timeseries data or Sequence data** - **3D tensors** of **shape (samples, timesteps, features)**.\n",
    "  <br><br>\n",
    "  - **Images** - **4D tensors** of **shape (samples, height, width, channels)** or **(samples, channels, height, width)**.\n",
    "  <br><br>\n",
    "  - **Video** - **5D tensors** of **shape (samples, frames, height, width, channels)** or **(samples, frames, channels, height, width)**.\n",
    "  \n",
    "<br>\n",
    "\n",
    "Here **channels** refer to the **number of color channels**, for example, **gray scale images** have only a **single color channel**.\n",
    "\n",
    "<br>\n",
    " \n",
    "<table bgcolor=\"white\">\n",
    "  <tr text-align=\"left\">\n",
    "    <th style=\"font-weight:bold; font-size:14px; text-align:center\">Timeseries Data</th>\n",
    "    <th style=\"font-weight:bold; font-size:14px; text-align:center\">Image Data</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <tr>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/timeseries.png\" width=\"500\" height=\"500\"/></td>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/imagedata.png\" width=\"500\" height=\"500\"/></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s6jZ4RkqlIl4"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mVGbohCNlIl5"
   },
   "source": [
    "<a id=section2></a>\n",
    "## 2. Vector Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KryX_CIllImF"
   },
   "source": [
    "- This is the **most common case**. \n",
    "\n",
    "\n",
    "- In such a dataset, **each single data point** can be **encoded as a vector**, and thus a **batch of data** will be **encoded as a 2D tensor** (that is, an **array of vectors**), where the **first axis is the samples axis** and the **second axis is the features axis**.\n",
    "\n",
    "\n",
    "- Let’s take a look at two **examples**:\n",
    "<br><br>\n",
    "  - An **actuarial dataset of people**, where we consider each **person’s age, ZIP code**, and **income. Each person** can be **characterized** as a **vector of 3 values**, and thus an entire **dataset of 100,000 people** can be **stored** in a **2D tensor of shape (100000, 3)**.\n",
    "<br><br>\n",
    "  - A dataset of text documents, where we **represent each document by** the **counts** of **how many times each word appears** in it (out of a **dictionary of 20,000 common words**). **Each document** can be **encoded** as a **vector of 20,000 values (one count per word in the dictionary)**, and thus an entire **dataset of 500 documents** can be **stored** in a **tensor of shape (500, 20000)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yGgGoUwwlImG"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7mbVQqdVlImH"
   },
   "source": [
    "<a id=section3></a>\n",
    "## 3. Tensor Operations in a Nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T8OrvuKrlImI"
   },
   "source": [
    "- Much as any **computer program** can be ultimately **reduced** to a **small set** of **binary operations on binary inputs** (**AND, OR, NOR,** and so on), all **transformations learned by deep neural networks** can be **reduced to** a handful of **tensor operations applied to tensors of numeric data**. \n",
    "\n",
    "\n",
    "- For instance, **it’s possible** to **add tensors**, **multiply tensors**, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0Xx4N6qlImJ"
   },
   "source": [
    "<a id=section301></a>\n",
    "### 3.1 Element-wise Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeScqRW8lImL"
   },
   "source": [
    "- **Element-wise operations**: **operations** that are **applied independently** to **each entry** in the **tensors** being considered. Examples are **relu** operation and **addition**.\n",
    "\n",
    "\n",
    "- This means these **operations** are **highly amenable to massively parallel implementations** (vectorized implementations, a term that comes from the vector processor supercomputer architecture from the 1970–1990 period). \n",
    "\n",
    "<br> \n",
    "- If you want **to write** a naive **Python implementation** of an **element-wise operation**, you **use a for loop**, as in this naive implementation of an **element-wise addition**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "YgHm7EfSlImP"
   },
   "source": [
    "# Element-wise addition\n",
    "\n",
    "def naive_add(x, y):\n",
    "    assert len(x.shape) == 2                  # x is a 2D Numpy tensor\n",
    "    assert x.shape == y.shape\n",
    "    x = x.copy()                              # Avoid overwriting the input tensor\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WpSfK3mNlImQ"
   },
   "source": [
    "- On the **same principle**, you can do **element-wise multiplication**, **subtraction**, and **so on**. \n",
    "\n",
    "\n",
    "- In practice, **when dealing with Numpy arrays**, these **operation**s are **available** as well **optimized built-in Numpy functions**, which themselves **delegate** the **heavy lifting** to a **Basic Linear Algebra Subprograms** (BLAS) implementation if you have one installed (which you should). \n",
    "\n",
    "\n",
    "- **BLAS** are **low-level, highly parallel, efficient tensor-manipulation routines** that are typically **implemented** in **Fortran or C**. \n",
    "\n",
    "<br> \n",
    "- So, in **Numpy**, you can do the **following element-wise operation**, and it will be **blazing fast**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "L2V_aPtZlImR",
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Element-wise addition\n",
    "\n",
    "z = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "B_eWfsdklImU"
   },
   "source": [
    "# Element-wise maximum operation (relu)\n",
    "\n",
    "z = np.maximum(z, 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vZiHNYU1lImV"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RhBcw0rjlImW"
   },
   "source": [
    "<a id=section302></a>\n",
    "### 3.2 Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h8y8FlXXlImX"
   },
   "source": [
    "- Our earlier naive implementation of **naive_add** only **supports** the **addition** of **2D tensors with identical shapes**. But **what happens** with **addition** when the **shapes** of the **two tensors being added differ**?\n",
    "\n",
    "\n",
    "- When possible, and if there’s no ambiguity, the **smaller tensor** will be **broadcasted** to **match the shape** of the **larger tensor**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "raThH7dRlImY"
   },
   "source": [
    "- **Broadcasting** consists of **two steps**:\n",
    "  \n",
    "  1. **Axes** (called **broadcast axes**) are **added to** the **smaller tensor** to **match** the **ndim** of the **larger tensor**.\n",
    "<br><br>  \n",
    "  2. The **smaller tensor** is **repeated alongside** these **new axes** to **match** the **full shape** of the **larger tensor**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U6v4XnuBlImZ"
   },
   "source": [
    "- Let’s look at a **concrete example**. \n",
    "\n",
    "  - Consider **X** with **shape (32, 10)** and **y** with **shape (10,)**. \n",
    "  \n",
    "  - First, we **add** an **empty first axis to y**, whose **shape becomes (1, 10)**. \n",
    "  \n",
    "  - Then, we **repeat y 32 times alongside** this **new axis**, so that we **end up** with a **tensor Y with shape (32, 10)**, where **Y[i, :] == y for i in range(0, 32)**. \n",
    "  \n",
    "  - **At this point**, we can **proceed** to **add X and Y**, because **they have** the **same shape**.\n",
    "\n",
    "\n",
    "- In **terms of implementation**, **no new 2D tensor is created**, because that **would be** terribly **inefficient**. The **repetition operation is entirely virtual**: it **happens at the algorithmic level rather** than at the **memory level**. But thinking of the vector being repeated 10 times alongside a new axis is a helpful mental model. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/broadcasting.png\" width=\"850\" height=\"850\"/>\n",
    "\n",
    "<br> \n",
    "- Here’s what a **naive implementation** would look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "vQdzahINlIma"
   },
   "source": [
    "def naive_add_matrix_and_vector(x, y):\n",
    "    assert len(x.shape) == 2                  # x is a 2D Numpy tensor\n",
    "    assert len(y.shape) == 1                  # y is a Numpy vector\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    x = x.copy()                              # Avoid overwriting the input tensor\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k5ekB1mXlImb"
   },
   "source": [
    "- **With broadcasting**, you can generally **apply two-tensor element-wise operations** if **one tensor has shape (a, b, … n, n + 1, … m)** and the **other has shape (n, n + 1, … m)**. The **broadcasting will** then a**utomatically happen** for **axes a through n - 1**. \n",
    "\n",
    "<br> \n",
    "- The following example applies the **element-wise maximum operation** to **two tensors of different shapes** via **broadcasting**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "X5LHAndmlImc",
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "x = np.random.random((64, 3, 32, 10))        # x is a random tensor with shape (64, 3, 32, 10)\n",
    "y = np.random.random((32, 10))               # y is a random tensor with shape (32, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "zrXEkGeylIme"
   },
   "source": [
    "z = np.maximum(x, y)                         # The output z has shape (64, 3, 32, 10) like x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "evQ7aDQwlImg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3z8N-55lImj"
   },
   "source": [
    "<a id=section303></a>\n",
    "### 3.3 Tensor Dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0LWT6zn0lImk"
   },
   "source": [
    "- The ___dot___ **operation**, also called a **tensor product** (**not** to be confused with an **element-wise product**) is the **most common**, **most useful tensor operation**. \n",
    "\n",
    "\n",
    "- **Contrary to element-wise operations**, it **combines entries in the input tensors**.\n",
    "\n",
    "\n",
    "- An **element-wise product** is **done with** the __* operator__ in **Numpy, Keras, Theano**, and **TensorFlow**. \n",
    "\n",
    "<br> \n",
    "- ___dot___ uses a **different syntax** in **TensorFlow**, but **in both Numpy and Keras** it’s **done using** the **standard dot operator**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "lWk1BiXalImo"
   },
   "source": [
    "z = np.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItL02IVXlImp"
   },
   "source": [
    "- In **mathematical notation**, you’d note the **operation** with a **dot** (**.**): ```z = x . y```\n",
    "\n",
    "\n",
    "- **Mathematically, what does the dot operation do?** \n",
    "\n",
    "<br> \n",
    "- Let’s **start with** the **dot product** of **two vectors x and y**. It’s computed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "YG8wK5udlImq"
   },
   "source": [
    "def naive_vector_dot(x, y):\n",
    "    assert len(x.shape) == 1                  \n",
    "    assert len(y.shape) == 1              # x and y are Numpy vectors    \n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    z = 0.\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yw0QNsHOlImt"
   },
   "source": [
    "- The **dot product between two vectors** is a **scalar** and that **only vectors with** the **same number of elements** are **compatible for a dot product**. \n",
    "\n",
    "<br> \n",
    "- You can also take the **dot product between** a **matrix x** and a **vector y**, which **returns a vector** where the **coefficients are** the **dot products between y and the rows of x**. You **implement** it as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "4w8FR2c6lImu"
   },
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i] += x[i, j] * y[j]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6q7SVVD2lIm8"
   },
   "source": [
    "- Note that as soon as **one of the two tensors** has an **ndim greater than 1**, **dot is no longer symmetric**, which is to say that **dot(x, y) isn’t the same as dot(y, x)**.\n",
    "\n",
    "\n",
    "- Of course, a **dot product generalizes** to **tensors with an arbitrary number of axes**.\n",
    "\n",
    "\n",
    "- The **most common applications** may be the **dot product between two matrices**. \n",
    "  \n",
    "  - You can take the **dot product** of **two matrices x** and **y (dot(x, y))** if and only **if x.shape[1] == y.shape[0]**. \n",
    "  \n",
    "  - The **result** is a **matrix** with **shape (x.shape[0], y.shape[1])**, where the **coefficients are** the **vector products between the rows of x and the columns of y**. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/tensordot2.png\" width=\"400\" height=\"300\"/>\n",
    "\n",
    "\n",
    "- Here’s the **naive implementation**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "LPdJKnMwlIm8",
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "def naive_matrix_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            row_x = x[i, :]\n",
    "            column_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DRe7MkJulIm9"
   },
   "source": [
    "- To **understand dot-product shape compatibility**, it helps to **visualize the input** and **output tensors** by **aligning them** as shown in figure:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/tensordot3.png\" width=\"250\" height=\"250\"/>\n",
    "<br> \n",
    "\n",
    "- **x**, **y**, and **z** are **pictured as rectangles** (literal boxes of coefficients). \n",
    "\n",
    "\n",
    "- Because the **rows of x** and the **columns of y must have** the **same size**, it follows that the **width of x must match** the **height of y**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fuvw0pfLlInB"
   },
   "source": [
    "- You can take the **dot product between higher-dimensional tensors**, **following** the same **rules for shape compatibility** as outlined earlier for the 2D case:\n",
    "<br><br>\n",
    " - _(a, b, c, d)_ **.** _(d,)_ **-->** _(a, b, c)_\n",
    "<br><br>\n",
    " - _(a, b, c, d)_ **.** _(d, e)_ **-->** _(a, b, c, e)_\n",
    "<br><br>\n",
    " - And so on.\n",
    " \n",
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/tensordot.png\" width=\"500\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRByKj0BlInD"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LIa6z6hXlInF"
   },
   "source": [
    "<a id=section304></a>\n",
    "### 3.4 Tensor Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "620UNXhNlInM"
   },
   "source": [
    "- A **third type** of **tensor operation** that’s essential to understand is **tensor reshaping**.\n",
    "\n",
    "\n",
    "- **Reshaping a tensor** means **rearranging its rows and columns** to **match a target shape**.\n",
    "\n",
    "\n",
    "- Naturally, the **reshaped tensor has** the **same total number of coefficients** as the **initial tensor**. \n",
    "\n",
    "\n",
    "- **Reshaping** is best understood via simple **examples**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_jJCDP6olInU"
   },
   "outputs": [],
   "source": [
    "x = np.array([[0., 1.],\n",
    "              [2., 3.],\n",
    "              [4., 5.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "391Tp9RllInh",
    "outputId": "57fa3906-16c7-47a3-c14b-4a0e776238ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tp1dOOvVlInr",
    "outputId": "3a69d420-f8f8-4b4c-b271-8c41dd8790b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [5.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape((6, 1))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U1R1h2PalInu",
    "outputId": "3ab65dbd-db06-47b7-e4bd-e8239e8830ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2.],\n",
       "       [3., 4., 5.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape((2, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRXph4EulIny"
   },
   "source": [
    "- A **special case** of **reshaping** that’s commonly encountered is **transposition**. \n",
    "\n",
    "\n",
    "- **Transposing a matrix** means **exchanging its rows** and its **columns**, so that **x[i, :] becomes x[:, i]**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtMiO5vSlIn0",
    "outputId": "bc1cf364-68d6-4642-b3e3-789c3005a448"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 300)\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((300, 20))                   # Creates an all-zeros matrix of shape (300, 20)\n",
    "x = np.transpose(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GQoqWCenlIn7"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AinjIDRXlIn8"
   },
   "source": [
    "<a id=section305></a>\n",
    "### 3.5 A Geometric Interpretation of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2e4A-yh_lIn9"
   },
   "source": [
    "- You just learned that **neural networks consist entirely of chains of tensor operations** and that all of these **tensor operations** are just **geometric transformations of the input data**. \n",
    "\n",
    "\n",
    "- It follows that you can **interpret a neural network** as a **very complex geometric transformation in a high-dimensional space**, **implemented via** a **long series of simple steps**. \n",
    "\n",
    "\n",
    "- **In 3D**, the **following mental image** may **prove useful**. \n",
    "\n",
    " - **Imagine two sheets** of **colored paper**: **one red** and **one blue**. \n",
    "\n",
    " - **Put one on top of** the **other**. \n",
    " \n",
    " - Now **crumple them together** into a **small ball**. \n",
    " \n",
    " - That **crumpled paper ball** is your **input data**, and **each sheet of paper** is a **class of data in a classification problem**. \n",
    " \n",
    " - What a **neural network** (or any other machine-learning model) is **meant to do** is **figure out a transformation** of the **paper ball** that would **uncrumple i**t, so as **to make** the **two classes cleanly separable again**. \n",
    " \n",
    " - With **deep learning**, this would be **implemented** as a **series** of **simple transformations** of the **3D space**, such as **those you could apply** on the **paper ball with your fingers**, **one movement at a time**.\n",
    "<br><br>  \n",
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/deep_learning_interpretation.png\" width=\"800\" height=\"800\"/>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lO85ImXFlIn-"
   },
   "source": [
    "- **Uncrumpling paper balls** is what **machine learning is about**: **finding neat representations** for **complex, highly folded data manifolds**. \n",
    "\n",
    "\n",
    "- At this point, you should have a pretty good intuition as to **why deep learning excels** at this: **it takes** the **approach** of **incrementally decomposing** a **complicated geometric transformation** into a **long chain of elementary ones**, which is pretty much the **strategy** a **human would follow** to **uncrumple a paper ball**. \n",
    "\n",
    "\n",
    "- **Each layer** in a **deep network applies** a **transformation** that **disentangles the data a little** and a **deep stack of layers makes tractable** an **extremely complicated disentanglement process**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TJg5-bTxlIn_"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CBL46qIylIoA"
   },
   "source": [
    "<a id=section4></a>\n",
    "## 4. Basic Maths for Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGFLISlAlIoB"
   },
   "source": [
    "- **Each neural layer transforms** its **input data** as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "YGVrJXwXlIoC"
   },
   "source": [
    "output = relu(dot(W, input) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fi7WRLdRlIoD"
   },
   "source": [
    "- In this expression, **W** and **b** are **tensors** that are **attributes of the layer**. \n",
    "\n",
    "  - They’re **called** the **weights** or **trainable parameters** of the **layer** (the **kernel** and **bias attributes, respectively**). \n",
    " \n",
    "  - These **weights contain** the **information learned** by the **network** from **exposure to training data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DIOKp2J9lIoE"
   },
   "source": [
    "- Initially, these **weight matrices** are **filled** with **small random values** (a step **called random initialization**). \n",
    "\n",
    "\n",
    "- Of course, there’s **no reason to expect** that **relu(dot(W, input) + b)**, when **W and b are random**, will **yield** any **useful representations**. The **resulting representations** are **meaningless**, **but** they’re a **starting point**. \n",
    "\n",
    "\n",
    "- What comes **next** is to **gradually adjust** these **weights**, **based** on a **feedback signal**. This **gradual adjustment**, also **called training**, **is** basically **the learning that machine learning is all about**. \n",
    "\n",
    "<br> \n",
    "- This **happens within** what’s called **a training loop**, which works as follows. **Repeat these steps in a loop**, **as long as necessary**:\n",
    "\n",
    " 1. **Draw** a **batch of training samples x** and **corresponding targets y**.\n",
    "<br><br>\n",
    " 2. **Run** the **network on x** (a step **called** the **forward pass**) to **obtain predictions y_pred**.\n",
    "<br><br>\n",
    " 3. **Compute** the **loss of** the **network on** the **batch**, a **measure of** the **mismatch between y_pred and y**.\n",
    "<br><br>\n",
    " 4. **Update all weights** of the **network in** a **way** that **slightly reduces** the **loss on** this **batch**.\n",
    " \n",
    " \n",
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/gradient_descent.png\" width=\"700\" height=\"700\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OajuEcSWlIoE"
   },
   "source": [
    "- You’ll **eventually end up** with a **network that has** a **very low loss on** its **training data**: a **low mismatch between predictions y_pred** and **expected targets y**. \n",
    "\n",
    "\n",
    "- The **network** has **“learned” to map** its **inputs to correct targets**. \n",
    "\n",
    "\n",
    "- **From afar**, it may **look like magic**, but **when** you **reduce it to elementary step**s, it **turns out to be simple**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HOnBQJFDlIoF"
   },
   "source": [
    "- **Step 1** sounds easy enough - just **I/O code**. **Steps 2 and 3** are merely the **application of** a handful of **tensor operations**, so you could **implement these steps** purely **from what you learned** in the **previous section**. \n",
    "\n",
    "\n",
    "- The **difficult part** is **step 4**: **updating the network's weights**. **Given** an **individual weight coefficient** in the network, **how** can you **compute whether** the **coefficient should be increased** or **decreased**, and **by how much?**\n",
    "\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/GD.png\" width=\"500\" height=\"500\"/>\n",
    "<br> \n",
    "\n",
    "- One **naive solution** would be to **freeze all weights** in the **network except** the **one scalar coefficient** being **considered**, and **try different values** for this **coefficient**. **But such** an **approach** would be **horribly inefficient**, because **you’d need** to **compute two forward passes** (which are **expensive**) for **every individual coefficient** (of which there are many, usually **thousands** and sometimes **up to millions**). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eM5OOKg_lIoJ"
   },
   "source": [
    "- A much **better approach** is to **take advantage** of the fact that **all operations used** in the **network are differentiable**, and **compute** the **gradient of** the **loss with regard** to the **network’s coefficients**. \n",
    "\n",
    "\n",
    "- You can **then move the coefficients in** the **opposite direction from** the **gradient**, thus **decreasing the loss**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bITLv6YclIoK"
   },
   "source": [
    "<a id=section401></a>\n",
    "### 4.1 What’s a derivative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqanIzChlIoS"
   },
   "source": [
    "- Consider a continuous, smooth function **f(x) = y**, mapping a real number x to a new real number y. \n",
    "<br> \n",
    "  - Because the **function** is **continuous**, a **small change in x** can only **result in** a **small change in y**, that’s the intuition behind continuity. \n",
    "<br><br>  \n",
    "  - Let’s say you **increase x by** a **small factor epsilon_x**: this **results in** a **small epsilon_y change to y**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "JBlTqsJ6lIoU"
   },
   "source": [
    "f(x + epsilon_x) = y + epsilon_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VpcXj9K8lIoU"
   },
   "source": [
    "- In addition, because the **function is smooth** (its **curve doesn’t have any abrupt angles**), **when epsilon_x** is **small enough**, **around** a certain **point p**, it’s **possible to approximate f** as a **linear function** of **slope a**, so that **epsilon_y becomes a * epsilon_x**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "Ep9idl5VlIoV"
   },
   "source": [
    "f(x + epsilon_x) = y + a * epsilon_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xudGGDflIoW"
   },
   "source": [
    "- Obviously, this **linear approximation** is **valid** only **when x** is **close enough to p**. \n",
    "<br><br> \n",
    "  - The **slope a** is **called** the **derivative of f in p**. \n",
    "<br><br> \n",
    "  - If **a** is **negative**, it **means** a **small change of x around p** will **result** in a **decrease of f(x)**. \n",
    "<br><br> \n",
    "  - And if **a** is **positive**, a **small change in x** will **result in** an **increase of f(x)**. \n",
    "<br><br>   \n",
    "  - Further, the **absolute value of a** (the **magnitude of** the **derivative**) **tells** you **how quickly** this **increase or decrease will happen**.\n",
    "  \n",
    "  \n",
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/derivative.png\" width=\"600\" height=\"600\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h8Gw_zKGlIoX"
   },
   "source": [
    "- For every **differentiable function f(x)** (differentiable means **“can be derived”**: for example, smooth, continuous functions can be derived), there **exists** a **derivative function f'(x)** that **maps values** of **x** to the **slope of** the **local linear approximation of f** in those **points**. \n",
    "\n",
    "\n",
    "- For instance, the **derivative of cos(x)** is **-sin(x)**, the **derivative** of **f(x) = a * x** is **f'(x) = a**, and so on.\n",
    "\n",
    "\n",
    "- If you’re trying **to update x** by a **factor epsilon_x** in order **to minimize f(x)**, and you **know** the **derivative of f**, then your job is done: the **derivative completely describes how f(x) evolves** as you **change x**. If you want **to reduce** the **value of f(x)**, you just **need to move x** a little in the **opposite direction from the derivative**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pdqLEAfblIoY"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ytpj1_-LlIoZ"
   },
   "source": [
    "<a id=section402></a>\n",
    "### 4.2 Derivative of a Tensor Operation: the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JkqQMnRWlIoa"
   },
   "source": [
    "- A **gradient** is the **derivative** of a **tensor operation**. \n",
    "\n",
    "\n",
    "- It’s the **generalization** of the **concept of derivatives** to **functions of multidimensional inputs**: that is, to **functions** that **take tensors as inputs**.\n",
    "\n",
    "\n",
    "- **Consider** an **input vector x**, a **matrix W**, a **target y**, and a **loss function loss**. You can **use W to compute** a **target candidate y_pred**, and **compute** the **loss**, or **mismatch**, **between** the **target candidate y_pred** and the **target y**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "27ZE4DZ0lIod"
   },
   "source": [
    "y_pred = dot(W, x)\n",
    "loss_value = loss(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CS365lBolIoe"
   },
   "source": [
    "- **I**f the **data inputs x** and **y are frozen**, then **this** can be **interpreted** as a **function mapping values of W to loss values**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "KH6bChn0lIoe"
   },
   "source": [
    "loss_value = f(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FOr3_C7-lIof"
   },
   "source": [
    "- Let’s say the **current value** of **W is W0**. \n",
    "<br><br>\n",
    " - Then the **derivative of f** in the **point W0** is a **tensor gradient(f)(W0)** with the **same shape as W**, where **each coefficient gradient(f)(W0)[i, j] indicates** the **direction and magnitude** of the **change in loss_value** you **observe when modifying W0[i, j]**. \n",
    "<br><br> \n",
    " - That **tensor gradient(f)(W0)** is the **gradient** of the **function f(W) = loss_value in W0**.\n",
    "\n",
    "<br> \n",
    "- The **derivative** of a **function f(x)** of a **single coefficient** can be **interpreted** as the **slope of** the **curve of f**. Likewise, **gradient(f)(W0)** can be **interpreted as** the **tensor describing** the **curvature of f(W) around W0**.\n",
    "\n",
    "\n",
    "- For this reason, in much the same way that, **for** a **function f(x)**, you can **reduce** the **value of f(x)** by **moving x a little** in the **opposite direction from** the **derivative**, with a **function f(W) of a tensor**, you can **reduce f(W)** by **moving W** in the **opposite direction from the gradient**: \n",
    "<br><br>\n",
    "  - For example, **W1 = W0 - step * gradient(f)(W0)** (where **step** is a **small scaling factor**). \n",
    "<br><br>  \n",
    "  - That **means going against** the **curvature**, **which** intuitively should **put** you **lower on the curve**. \n",
    "<br><br>  \n",
    "  - Note that the **scaling factor step** is **needed because gradient(f)(W0)** only **approximates** the **curvature when** you’re **close to W0**, so you **don’t want** to **get too far from W0**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m6k7EFaYlIoj"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NsgNijmClIom"
   },
   "source": [
    "<a id=section403></a>\n",
    "### 4.3 Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3msJ562lIoo"
   },
   "source": [
    "- Given a **differentiable function**, it’s **theoretically possible** to **find its minimum analytically**: it’s known that a **function’s minimum** is a **point where** the **derivative is 0**, so all you have to do is **find all** the **points where** the **derivative goes to 0** and **check for which** of these **points** the **function has** the **lowest value**.\n",
    "\n",
    "\n",
    "- **Applied to** a **neural network**, that **means finding analytically** the **combination of weight values** that **yields** the **smallest possible loss function**. \n",
    "<br><br>\n",
    "  - This can be **done** by **solving** the **equation gradient(f)(W) = 0 for W**. \n",
    "<br><br>  \n",
    "  - This is a **polynomial equation** of **N variables**, where **N** is the **number of coefficients** in the **network**. \n",
    "<br><br>  \n",
    "  - Although it would be **possible to solve** such an **equation for N = 2** or **N = 3**, **doing** so **is intractable** for **real neural networks**, where the **number of parameters** is **never less than** a **few thousand** and **can often be** several **tens of millions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2b1_w39klIot"
   },
   "source": [
    "- Instead, you can **use** the **four-step algorithm** outlined at the beginning of this section: **modify** the **parameters little by little based on** the **current loss value on** a **random batch of data**. \n",
    "\n",
    "\n",
    "- Because you’re **dealing with** a **differentiable function**, you can **compute its gradient**, which **gives** you **an efficient way** to **implement step 4**. \n",
    "\n",
    "<br> \n",
    "- If you **update the weights in** the **opposite direction from** the **gradient**, the **loss will be** a **little less every time**:\n",
    "<br> \n",
    "  1. **Draw** a **batch of training samples x** and **corresponding targets y**.\n",
    "<br><br>    \n",
    "  2. **Run** the **network on x** to **obtain predictions y_pred**.\n",
    "<br><br>    \n",
    "  3. **Compute** the **loss of** the **network on** the **batch**, a **measure of** the **mismatch between y_pred and y**.\n",
    "<br><br>    \n",
    "  4. **Compute** the **gradient of** the **loss** with **regard to** the **network’s parameters** (a **backward pass**).\n",
    "<br><br>   \n",
    "  5. **Move** the **parameters** a **little in** the **opposite direction from** the **gradient**. \n",
    "<br><br>   \n",
    "    - For example **W -= step * gradient**, thus **reducing** the **loss on** the **batch a bit**.\n",
    "\n",
    "<br> \n",
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/SGD.png\" width=\"700\" height=\"700\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWGI9Z22lIou"
   },
   "source": [
    "- This is called **mini-batch stochastic gradient descent** (**minibatch SGD**). \n",
    "\n",
    "\n",
    "- The **term stochastic refers** to the fact that **each batch of data** is **drawn at random** (**stochastic** is a **scientific synonym** of **random**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8WcbmmOllIoy"
   },
   "source": [
    "- It’s **important to pick** a **reasonable value for** the **step factor (a.k.a. learning rate)**. \n",
    "<br><br> \n",
    "  - **If** it’s **too small**, the **descent down the curve** will **take many iterations**, and it **could get stuck in** a **local minimum**. \n",
    "<br><br>   \n",
    "  - **If step** is **too large**, your **updates may end up taking** you to **completely random locations on** the **curve**. \n",
    "  \n",
    "<br> \n",
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/learning_rate.png\" width=\"500\" height=\"500\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jSkB5qZKlIoz"
   },
   "source": [
    "- Note that a **variant of** the **mini-batch SGD algorithm** would be to **draw a single sample** and **target at each iteration**, **rather** than **drawing a batch of data**. This **would be true SGD** (as **opposed to mini-batch SGD**). \n",
    "\n",
    "\n",
    "- **Alternatively**, going to **the opposite extreme**, you could **run every step on all data available**, which is **called batch SGD**. **Each update** would then be **more accurate**, **but far more expensive**. \n",
    "\n",
    "\n",
    "- The **efficient compromise between** these **two extremes is** to **use mini-batches** of **reasonable size**.\n",
    "\n",
    "<br> \n",
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/SGD_2D5.jpg\" width=\"800\" height=\"800\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZyrhMwMlIo0"
   },
   "source": [
    "- Additionally, **there exist multiple variants of SGD** that **differ** by **taking into account previous weight updates** when **computing** the **next weight update**, **rather than** just **looking at** the **current value of** the **gradients**. \n",
    "\n",
    "\n",
    "- **There is**, for instance, **SGD with momentum**, as well as **Adagrad**, **RMSProp**, and several others. **Such variants** are **known as optimization methods** or **optimizers**. \n",
    "\n",
    "\n",
    "- In particular, the **concept of momentum**, which is **used in many** of these **variants**, deserves your attention. \n",
    "\n",
    "<br> \n",
    "- **Momentum addresses two issues** with **SGD**: **convergence speed** and **local minima**.\n",
    "\n",
    "  \n",
    "<br> \n",
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/minimas2.png\" width=\"600\" height=\"600\"/>\n",
    "<br><br> \n",
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/minimas.png\" width=\"700\" height=\"700\"/>\n",
    "<br>\n",
    "\n",
    "<br><br> \n",
    "  - As you can see, **around a certain parameter value**, there is a **local minimum**: **around that point**, **moving left** would **result in** the **loss increasing**, **but so would moving right**. \n",
    "<br><br>   \n",
    "  - **If** the **parameter under consideration** were being **optimized via SGD with** a **small learning rate**, **then** the **optimization process** would **get stuck a**t the **local minimum instead** of **making its way to** the **global minimum**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9L7x2O5dlIo3"
   },
   "source": [
    "- You can **avoid such issues** by **using momentum**, which **draws inspiration from physics**. \n",
    "<br><br> \n",
    "  - A useful mental image here is to **think of** the **optimization process as** a **small ball rolling down** the **loss curve**. \n",
    "\n",
    "<br> \n",
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/momentum.png\" width=\"600\" height=\"600\"/>\n",
    "<br>\n",
    "\n",
    "  - **If it has enough momentum**, the **ball won’t get stuck in** a **ravine** and **will end up at** the **global minimum**. \n",
    "<br><br>  \n",
    "  - **Momentum** is **implemented** by **moving the ball** at **each step based not only** on the **current slope value** (**current acceleration**) **but also** on **the current velocity** (**resulting from past acceleration**). \n",
    "<br><br>   \n",
    "  - **In practice**, this **means updating** the **parameter w based not onl**y on the **current gradient value but also** on the **previous parameter update**, such as in this **naive implementation**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "YpkDgLa1lIo4"
   },
   "source": [
    "past_velocity = 0.\n",
    "momentum = 0.1                                                           # Constant momentum factor\n",
    "while loss > 0.01:                                                       # Optimization loop \n",
    "    w, loss, gradient = get_current_parameters()\n",
    "    velocity = past_velocity * momentum + learning_rate * gradient\n",
    "    w = w + momentum * velocity - learning_rate * gradient\n",
    "    past_velocity = velocity\n",
    "    update_parameter(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bFb0RMzolIo6"
   },
   "source": [
    "<br> \n",
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/momentum1.png\" width=\"500\" height=\"500\"/>\n",
    "<br>\n",
    "<br> \n",
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/momentum2.png\" width=\"600\" height=\"600\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUvo4GhElIo7"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pWFVo7ctlIo9"
   },
   "source": [
    "<a id=section404></a>\n",
    "### 4.4 Chaining Derivatives: the Backpropagation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r11NiFldlIo-"
   },
   "source": [
    "- **In** the **previous algorithm**, we casually **assumed that** because a **function is differentiable**, we can **explicitly compute its derivative**. \n",
    "\n",
    "\n",
    "- **In practice**, a **neural network function consists** of **many tensor operations chained together**, **each** of which **has a simple**, **known derivative**. \n",
    "\n",
    "\n",
    "- For instance, this is a **network f composed** of **three tensor operations**, **a**, **b**, and **c**, with **weight matrices W1**, **W2**, and **W3**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "GS-4pgPSlIo_"
   },
   "source": [
    "f(W1, W2, W3) = a(W1, b(W2, c(W3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V62NNPFwlIpb"
   },
   "source": [
    "- **Calculus** tells us that **such** a **chain of functions** can be **derived using** the following **identity**, **called** the **chain rule**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "ESN8NPAjlIpd"
   },
   "source": [
    "f(g(x)) = f'(g(x)) * g'(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xIPkTdZlIpe"
   },
   "source": [
    "- **Applying** the **chain rule to** the **computation of** the **gradient values** of a **neural network gives rise** to an **algorithm called Backpropagation** (also sometimes called **reverse-mode differentiation**). \n",
    "\n",
    "\n",
    "- **Backpropagation starts with** the **final loss value** and **works backward from** the **top layers to** the **bottom layers**, **applying** the **chain rule to compute** the **contribution that each parameter had in** the **loss value**.\n",
    "\n",
    "<br> \n",
    "<img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/backpropagation.png\" width=\"500\" height=\"500\"/>\n",
    "<br>\n",
    "\n",
    "- Nowadays, and for years to come, **people** will **implement networks in modern frameworks** that are **capable of symbolic differentiation**, such as **TensorFlow**. \n",
    "<br><br> \n",
    "  - This means that, **given a chain of operations with** a **known derivative**, **they can compute** a g**radient function for** the **chain** (by **applying** the **chain rule**) **that maps network parameter values** to **gradient values**. \n",
    "<br><br> \n",
    "  - **When** you **have access to such** a **function**, the **backward pass** is **reduced to** a **call to** this **gradient function**. \n",
    "<br><br> \n",
    "  - **Thanks** to **symbolic differentiation**, you’ll **never have to implement** the **Backpropagation algorithm by hand**. \n",
    "<br><br>  \n",
    "  - For this reason, we **won’t waste your time** and your **focus on deriving** the **exact formulation of** the **Backpropagation algorithm** in these pages.\n",
    "<br><br>  \n",
    "  - **All you need** is a **good understanding** of **how gradient-based optimization works**."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Maths behind AI - Simplified.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
