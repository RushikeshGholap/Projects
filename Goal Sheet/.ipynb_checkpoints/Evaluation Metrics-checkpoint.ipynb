{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clustering Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<p>(Some basic theory)</p>\n",
    "<h3>Information I(x)</h3>\n",
    "\n",
    "<p><b><u>Definition</u></b> - I(x) is the measure of 'knowledge' gained from occurance of an event's outcome.</p>\n",
    "\n",
    "<p><b><u>Explanation</u></b> - Consider 2 events, Neo takes the blue pill; Neo takes the red pill. The probability that Neo takes the red pill is already high since he is interested in knowing the truth. While the probabilty he takes the blue pill is lower. Therefore, the information that you gain if Neo takes the blue pill will be more, since its a less likely event to occur and thus more 'shocking'. This means that information is a function of probability, I(px) where px is probability of event x.</p>\n",
    "\n",
    "<p>Now, consider 2 completely independent events, x and y. The probability of both these events occuring is px.py. So intuitively, we can say that I(px.py) = I(px) + I(py). This is also a property of the log function. So we can say that I is a logarithmic function of probability of an event, and because I(px) increases and probability decreases, it needs to be negative.</p>\n",
    "\n",
    "<p>Therefore <b>I(x) = -log(px)</b></p>\n",
    "\n",
    "<h3>Entropy H(x)</h3>\n",
    "\n",
    "<p><b><u>Definition</u></b> - Entropy is the amount of information held in a random variable/event. It can also be defined as the expected measure of amount of information gained when observing a random variable x.</p>\n",
    "\n",
    "<p><b><u>Explanation</u></b> - Entropy is measured by multiplying the information of each event by their probability of occurance, and then summing them up. Its roughly a weighted average where the weight is the probability of occurance of the event.</p>\n",
    "\n",
    "<p>Therefore <b>H(x) = Σ(pi.I(pi)) = -Σ(pi.log(pi))</b></p>\n",
    "\n",
    "<br>\n",
    "\n",
    "<p><b>CRUX -</b>The objective usually is to maximize entropy, which means taking first order derivative of H(x) and equating it to 0. This yields that the entropy of a random variable is maximized when each of the possible outcomes are equally likely. Also, for intuition, if probability of an outcome is low, then information gained from observing that event is high. Entropy of that event (which looks at weighted information of all the outcomes) will be calculated accordingly, and it maximum if both outcomes have equal probability of occurance.</p>\n",
    "\n",
    "<ul>\n",
    "<li>https://www.youtube.com/watch?v=1oOP0sTQ9XI</li>\n",
    "<li>http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html#sklearn.metrics.mutual_info_score</li>\n",
    "<li>https://en.wikipedia.org/wiki/Mutual_information</li>\n",
    "<li>https://en.wikipedia.org/wiki/Adjusted_mutual_information</li>\n",
    "<li>https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Mutual Information MI(x,y)\n",
    "\n",
    "<b><u>Definition</u></b> - It is a measure of the mutual dependence between the two variables/events. It quantifies the \"amount of information\" obtained about one random variable/event, through the other random variable/event.\n",
    "\n",
    "<b><u>Explanation</u></b> - Intuitively it can be understood as the intersection between entropy of variable x and variable y. Refer this <a href=https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Entropy-mutual-information-relative-entropy-relation-diagram.svg/744px-Entropy-mutual-information-relative-entropy-relation-diagram.svg.png>image</a>. Its not critical to understand it mathematically.\n",
    "\n",
    "1. Its used to compare a given clustering with another (which could be a supervised clustering made to gauge efficiency).\n",
    "2. Mutual information score doesnt have any upper and changes its scale when number of clusterings is increased or decreased.\n",
    "3. Its a non-negative number always.\n",
    "4. Its also symmetrics, meaning order of clusterings given to the function dont matter.\n",
    "\n",
    "Its important to note that it functions quite similarly to a correlation measure in the fact that it doesn't depend on the absolute values of the labels. Mutual information of (a,a) and Mutual information of (a,b) is same as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69314718056\n",
      "0.69314718056\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Example with 2 sets of clusterings..\n",
    "a = [1,1,1,0,0,0]\n",
    "b = [2,2,2,3,3,3]\n",
    "\n",
    "d = [0,0,0,0,0,0]\n",
    "e = [0,1,2,3,4,5]\n",
    "\n",
    "print(metrics.mutual_info_score(a,a))\n",
    "print(metrics.mutual_info_score(a,b))\n",
    "print(metrics.mutual_info_score(d,e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Normalized Mutual Information NMI(x,y)\n",
    "\n",
    "<b><u>Definition</u></b> - Its a normalized variant of Mutual Information.\n",
    "\n",
    "<b><u>Explanation</u></b> - NMI is an normalization of the MI score to scale the results between 0 (no mutual information) and 1 (perfect correlation). In this function, mutual information is normalized by sqrt(H(x).H(y))\n",
    "\n",
    "1. It is bounded between 0 to 1\n",
    "2. NMI doesn't take into account 'chance', so adjusted_mutual_score is prefered instead of this.\n",
    "3. NMI is symmetric and independent of absolute values, similar to MI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.0817041659455\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Example with different sets of clusterings..\n",
    "a = [1,1,1,0,0,0]\n",
    "b = [2,2,2,3,3,3]\n",
    "c = [1,0,1,0,1,0]\n",
    "\n",
    "d = [0,0,0,0,0,0]\n",
    "e = [0,1,2,3,4,5]\n",
    "\n",
    "print(metrics.normalized_mutual_info_score(a,a))\n",
    "print(metrics.normalized_mutual_info_score(a,b))\n",
    "print(metrics.normalized_mutual_info_score(a,c))\n",
    "print(metrics.normalized_mutual_info_score(d,e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Adjusted Mutual Information AMI(x,y)\n",
    "\n",
    "<b><u>Definition</u></b> - Its a adjusted variant of MI. It corrects the effect of agreement solely due to chance between clusterings.\n",
    "\n",
    "<b><u>Explanation</u></b> - Non-Adjusted measures show a dependency between number of clusters and number of samples. Adjusted measures fix this issue and hence are a better measure of stability of clustering algorithms. AMI accounts for the fact that the MI is generally higher for two clusterings with a larger number of clusters, regardless of whether there is actually more information shared.\n",
    "\n",
    "1. The AMI returns a value of 1 when the two partitions are identical (ie perfectly matched). Random partitions (independent labellings) have an expected AMI around 0 on average hence can be negative. These can be interpreted as 0 however.\n",
    "2. AMI is symmetric and independent of absolute values, similar to MI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "-0.111111111111\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Example with different sets of clusterings.\n",
    "a = [1,1,1,0,0,0]\n",
    "b = [2,2,2,3,3,3]\n",
    "c = [1,0,1,0,1,0]\n",
    "\n",
    "d = [0,0,0,0,0,0]\n",
    "e = [0,1,2,3,4,5]\n",
    "\n",
    "print(metrics.adjusted_mutual_info_score(a,a))\n",
    "print(metrics.adjusted_mutual_info_score(a,b))\n",
    "print(metrics.adjusted_mutual_info_score(a,c))\n",
    "print(metrics.adjusted_mutual_info_score(d,e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Adjusted Rand Score\n",
    "\n",
    "<b><u>Definition</u></b> - Adjusted Rand Score is a variation of Rand Index, adjusted for chance. \n",
    "\n",
    "<b><u>Explanation</u></b> - Rand index represents the frequency of occurrence of agreements over the total pairs of randomly selected data points. The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings. The Rand index lies between 0 and 1. When two clusterings agree perfectly, the Rand index achieves the maximum value 1. \n",
    "\n",
    "Non-Adjusted measures show a dependency between number of clusters and number of samples. Adjusted measures fix this issue and hence are a better measure of stability of clustering algorithms. This is fixed by adjusted Rand Index, in a similar matter to AMI. The adjusted Rand index has the maximum value 1, and its expected value is 0 in the case of random clusters. A larger adjusted Rand index means a higher agreement between two partitions. The adjusted Rand index is recommended for measuring agreement even when the partitions compared have different numbers of clusters.\n",
    "\n",
    "\n",
    "<p>To summarize, <b>RI = (TP+TN)/(TP+TN+FP+FN)</b></p>\n",
    "<p>The general method of adjusting for change is roughtly, <b>ARI = (Index - ExpectedIndex) / (max(Index) - ExpectedIndex)</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<p>References-</p>\n",
    "<ul>\n",
    "<li>http://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html</li>\n",
    "<li>https://en.wikipedia.org/wiki/Rand_index</li>\n",
    "<li>https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html</li>\n",
    "<li>http://scikit-learn.org/stable/auto_examples/cluster/plot_adjusted_for_chance_measures.html</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "-0.111111111111\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Example with different sets of clusterings.\n",
    "a = [1,1,1,0,0,0]\n",
    "b = [2,2,2,3,3,3]\n",
    "c = [1,0,1,0,1,0]\n",
    "\n",
    "d = [0,0,0,0,0,0]\n",
    "e = [0,1,2,3,4,5]\n",
    "\n",
    "print(metrics.adjusted_rand_score(a,a))\n",
    "print(metrics.adjusted_rand_score(a,b))\n",
    "print(metrics.adjusted_rand_score(a,c))\n",
    "print(metrics.adjusted_rand_score(d,e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4\n",
    "\n",
    "<b><u>Definition</u></b> - \n",
    "\n",
    "<b><u>Explanation</u></b> - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4\n",
    "\n",
    "<b><u>Definition</u></b> - \n",
    "\n",
    "<b><u>Explanation</u></b> - "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
