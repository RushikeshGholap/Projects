{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "Its short for Bootstrap-Aggregation. The idea is to train classifiers on random sets of samples from the training data which is called bootstrapping. Then a voting mechanism aggregates results from these weak classifiers to make a much stronger one.\n",
    "\n",
    "1. Create n 'bags' of training data by randomly (with replacement) selecting samples from training data\n",
    "2. Fit a weak model on each of the bags to get n models\n",
    "3. 'Combine' the predictions of each model to predict\n",
    "\n",
    "#### How to combine predictions?\n",
    "\n",
    "For regression you can do averages, for classifications you can take a vote or average class probabilities.\n",
    "\n",
    "#### Why does it work?\n",
    "\n",
    "Imagine n features represented as $Z, Z_1, Z_2, ..., Z_n$ each with a mean of $\\mu$ and variance of $\\sigma^2$. When you take the average of the features, what you end up with is $Z_M$ which still has a mean of $\\mu$ BUT its variance now becomes $\\sigma^2/n$, thus shrinking the variance while keeping the same mean. This same analogy is used in prediction functions.\n",
    "\n",
    "So if we have N independent training sets, fit N classifiers, then take average of the predictions, our ensemble model still has the expected mean prediction, but the variance of predictions reduces, making the final model much stronger than the individual weak learners.\n",
    "\n",
    "Since we don't have independent training sets, we bootstrap the same training data with replacement, which doesn't give us the full reduction in variance. \n",
    "\n",
    "#### Evaluating the ensemble model?\n",
    "\n",
    "Let's say that each classifier is trained on about 63% data. The remaining 37% data is called out-of-bag samples (OOB). These can be use to \n",
    "\n",
    "So for each sample $X_i$ we can evaluate the set of models for which $X_i$ was a OOB sample. Then for only this set of models, vote/average their predictions, compare to actual and calculate the OOB error estimate of the model. This approach is quite similar to cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit, X_eval, Y_fit, Y_test = model_selection.train_test_split(X,Y,test_size = 0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=5, random_state=7, shuffle=False)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=5,random_state=7)\n",
    "kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=100, n_jobs=None, oob_score=True,\n",
       "         random_state=7, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees,random_state=seed, oob_score=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=100, n_jobs=None, oob_score=True,\n",
       "         random_state=7, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_fit, Y_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9523809523809523"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.95238095, 1.        , 0.9047619 , 0.85714286])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model_selection.cross_val_score(model, X_fit, Y_fit, cv=kfold)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  0  accuracy  1.0\n",
      "model  1  accuracy  0.9523809523809523\n",
      "model  2  accuracy  1.0\n",
      "model  3  accuracy  0.9047619047619048\n",
      "model  4  accuracy  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "x = [print(\"model \",i,\" accuracy \",results[i]) for i in range(0,len(results))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy  0.9428571428571428\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy \",results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Its a sequential way of aggregating multiple weak classifiers. A weak classifier is built on the data, and then the misclassified samples are given higher weightage before training another weak classifier. This compensates for the weaknesses of the previous model and builds a stronger one. This continues till you have a powerful model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit, X_eval, Y_fit, Y_test = model_selection.train_test_split(X,Y,test_size = 0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "          learning_rate=0.1, n_estimators=25, random_state=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AdaBoostClassifier(base_estimator=cart, n_estimators=num_trees, learning_rate=0.1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object BaseWeightBoosting.staged_score at 0x107c5c620>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.staged_score(X_fit,Y_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = model.predict(X_eval)\n",
    "nnz = np.float(np.shape(Y_test)[0] - np.count_nonzero(pred_label - Y_test))\n",
    "acc = 100*nnz/np.shape(Y_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.66666666666667"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "Multiple weak/strong classifiers are created independently on the same data. Then their predictions are used as features to train a logistic classifier which weights each feature (predictions from various models) and creates a powerful model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data[:,1:3], \n",
    "                                                    iris.target, \n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateAccuracy(y_actual,pred_label):\n",
    "    nnz = np.shape(y_actual)[0] - np.count_nonzero(pred_label - y_actual)\n",
    "    acc = 100*nnz/float(np.shape(y_actual)[0])\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models Trained!\n"
     ]
    }
   ],
   "source": [
    "clf1 = KNeighborsClassifier(n_neighbors=2)\n",
    "clf2 = RandomForestClassifier(n_estimators=2, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "\n",
    "clf1.fit(X_train,y_train)\n",
    "clf2.fit(X_train,y_train)\n",
    "clf3.fit(X_train,y_train)\n",
    "print('Models Trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Model Accuracy: 92.5\n",
      "RF Model Accuracy 95.83333333333333\n",
      "NB Model Accuracy 90.83333333333333\n"
     ]
    }
   ],
   "source": [
    "f1 = clf1.predict(X_train)\n",
    "f2 = clf2.predict(X_train)\n",
    "f3 = clf3.predict(X_train)\n",
    "\n",
    "acc1 = CalculateAccuracy(y_train, f1)\n",
    "acc2 = CalculateAccuracy(y_train, f2)\n",
    "acc3 = CalculateAccuracy(y_train, f3)\n",
    "\n",
    "print('KNN Model Accuracy:',acc1)\n",
    "print('RF Model Accuracy',acc2)\n",
    "print('NB Model Accuracy',acc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta Model Trained!\n"
     ]
    }
   ],
   "source": [
    "f = [f1,f2,f3]\n",
    "f = np.transpose(f)\n",
    "lr.fit(f,y_train)\n",
    "\n",
    "print('Meta Model Trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta model accuracy: 95.83333333333333\n"
     ]
    }
   ],
   "source": [
    "final = lr.predict(f)\n",
    "acc4 = CalculateAccuracy(y_train,final)\n",
    "print('Meta model accuracy:', acc4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
