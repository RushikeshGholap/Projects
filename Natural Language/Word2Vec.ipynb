{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is a group of models used to transform text data into word-embeddings. Its more useful as compared to using Countvectorizer, TFIDFvectorizer etc as - \n",
    "\n",
    "1. It captures the context of the text and encodes semantics in some form\n",
    "2. It has fixed dimensional (hyperparameter) vectors which helps with computation\n",
    "3. These are usually pre-trained over vast amounts of data and used directly in other models\n",
    "\n",
    "The word vectors created by the models are positioned in the vector space such that the words that share common contexts in the corpus are located closer to each other in the space.\n",
    "\n",
    "The 2 main model architectures in word2vec are - \n",
    "\n",
    "1. CBOW (Continous Bag of words) - Predicts the current word based on the surrounding context words as input\n",
    "2. SG (Skip gram) - Predicts surrounding context words based on current word\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "<u>Training algo</u> - Heirarchial softmax is used when model seeks to maximize conditional log-likelihood and uses Huffman tree. This is better for infrequent words, and lower number of epochs. Negative Sampling is useful when minimizing the log-likelihood of sampled negative instances. Its better for frequent words, lower dimensional vectors and higher number of epochs.\n",
    "\n",
    "<u>Sub-Sampling</u> - Higher freq words have lesser information, this is a threshold to subsample them to increase training speed\n",
    "\n",
    "<u>Dimensionality</u> - Quality of embeddings increases with dimensionality but after a point the marginal gain will diminish.\n",
    "\n",
    "<u>Context window</u> - context window determines how many words before and after the current word will be its context words. Recommended is 10 for SG and 5 for CBOW.\n",
    "\n",
    "\n",
    "<img src='https://lilianweng.github.io/lil-log/assets/images/word2vec-skip-gram.png'></img>\n",
    "\n",
    "**SKIP GRAM ARCHITECURE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Word2Vec using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1a1e9fdb38>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Online training the model with more sentences\n",
    "model.build_vocab([[\"hello\", \"world\"],['hey','world']], update=True)\n",
    "model.train([[\"hello\", \"world\"],['hey','world']], total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.9072831e-03, -5.0078036e-04, -1.2205215e-03,  1.7031934e-03,\n",
       "       -8.7878905e-04,  3.2642719e-03,  1.6955030e-03,  1.8635712e-03,\n",
       "        6.0884620e-04,  3.6294644e-03, -5.1687384e-05, -6.7350105e-04,\n",
       "       -1.3855461e-03,  3.8039340e-03, -2.2583394e-03,  2.9560386e-03,\n",
       "       -3.5702975e-03,  2.1870460e-03,  2.5708969e-03,  1.3285197e-03,\n",
       "        1.8306220e-03,  2.4980409e-03, -2.2381789e-03,  4.3474101e-03,\n",
       "       -2.2699737e-03,  2.1978179e-03, -3.5987915e-03, -1.4745519e-03,\n",
       "        8.9427346e-04,  2.5238441e-03, -3.7145237e-03,  2.1708685e-03,\n",
       "       -3.0585675e-04, -1.9012406e-03,  1.7093649e-03,  3.2643725e-03,\n",
       "       -2.7753536e-03,  2.0877942e-03,  8.5415441e-04, -2.3626110e-03,\n",
       "       -1.7397344e-03, -2.4701545e-03, -4.7957557e-03, -2.7853139e-03,\n",
       "        1.9269293e-03,  5.1363495e-05,  2.0468340e-04, -1.8245459e-03,\n",
       "        5.4087868e-04, -3.0424343e-03,  2.0268066e-03,  4.8809272e-04,\n",
       "       -1.6237929e-03,  3.8381990e-03, -4.0810690e-03,  1.5460260e-03,\n",
       "       -4.3985727e-03, -3.5452677e-03, -3.3397174e-03, -1.3920651e-03,\n",
       "        4.9285358e-03,  3.7491261e-03,  1.8839112e-03,  2.0496806e-03,\n",
       "        1.3024154e-03,  2.1044810e-03, -1.1049281e-03,  1.7161814e-03,\n",
       "        2.8153153e-03, -1.4844165e-03, -3.4058554e-04, -2.5665048e-03,\n",
       "        2.7391682e-03,  4.5176051e-03,  1.7721182e-03,  3.2947089e-03,\n",
       "       -1.8655484e-03, -3.9603976e-03, -2.0407976e-03, -4.2551905e-03,\n",
       "        3.6924481e-03, -3.4322285e-03, -1.1118595e-03,  3.4730271e-03,\n",
       "        3.1616904e-03, -4.5024250e-03, -2.7456582e-03, -5.9045036e-04,\n",
       "        4.6392018e-04, -5.7671092e-05,  4.1115852e-03,  2.0864119e-03,\n",
       "       -2.7867905e-03, -3.6722471e-03, -5.2827282e-04, -3.6018330e-03,\n",
       "        1.7772163e-03,  2.3061619e-03,  3.7336501e-03,  1.6083665e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['world']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First download the pre-trained word embeddings\n",
    "#### link - https://github.com/RaRe-Technologies/gensim-data ####\n",
    "\n",
    "#Load pre-trained model\n",
    "model = gensim.models.Word2Vec.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model.wv['computer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can now be used for creating sentence vectors by taking average (or weighted average with tfidf), or fed into a sequential model to create sequence representations with context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {}\n",
    "settings['n'] = 5                     #Dimensionality of word vectors\n",
    "settings['window_size'] = 2           #Context window\n",
    "settings['min_count'] = 0             #min word count\n",
    "settings['epochs'] = 5000             #training epochs\n",
    "settings['neg_samp'] = 10             #number of negative words to use during training\n",
    "settings['learning_rate'] = 0.01      #learning rate\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have you heard the word the word about the bird', 'hello world word']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['have you heard the word the word about the bird', 'hello world word']\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cnt = CountVectorizer(stop_words=None)\n",
    "cnt.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(corpus):\n",
    "    X, y = [],[]\n",
    "    for sentence in corpus:\n",
    "        sentence = sentence.split()\n",
    "        len_sent = len(sentence)\n",
    "        \n",
    "        for i in range(len_sent):\n",
    "            \n",
    "            #Get current word\n",
    "            current_word = get_onehot(sentence[i])\n",
    "            \n",
    "            #get context words\n",
    "            context_words = []\n",
    "            for j in range(i-settings['window_size'], i+settings['window_size']+1):\n",
    "                if j!=i and j<=len_sent-1 and j>=0:\n",
    "                    context_words.append(get_onehot(sentence[j]))\n",
    "            X.append(current_word)\n",
    "            y.append(context_words)\n",
    "            \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_training_data(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array(['have'], dtype='<U5')],\n",
       " [array(['you'], dtype='<U5')],\n",
       " [array(['heard'], dtype='<U5')],\n",
       " [array(['the'], dtype='<U5')],\n",
       " [array(['word'], dtype='<U5')],\n",
       " [array(['the'], dtype='<U5')],\n",
       " [array(['word'], dtype='<U5')],\n",
       " [array(['about'], dtype='<U5')],\n",
       " [array(['the'], dtype='<U5')],\n",
       " [array(['bird'], dtype='<U5')],\n",
       " [array(['hello'], dtype='<U5')],\n",
       " [array(['world'], dtype='<U5')],\n",
       " [array(['word'], dtype='<U5')]]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cnt.inverse_transform(i) for i in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([0, 0, 0, 0, 0, 0, 0, 0, 1]), array([0, 0, 0, 1, 0, 0, 0, 0, 0])],\n",
       " [array([0, 0, 1, 0, 0, 0, 0, 0, 0]),\n",
       "  array([0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
       "  array([0, 0, 0, 0, 0, 1, 0, 0, 0])],\n",
       " [array([0, 0, 1, 0, 0, 0, 0, 0, 0]),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
       "  array([0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       "  array([0, 0, 0, 0, 0, 0, 1, 0, 0])],\n",
       " [array([0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
       "  array([0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
       "  array([0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
       "  array([0, 0, 0, 0, 0, 1, 0, 0, 0])],\n",
       " [array([0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
       "  array([0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       "  array([0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       "  array([0, 0, 0, 0, 0, 0, 1, 0, 0])],\n",
       " [array([0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       "  array([0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
       "  array([0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
       "  array([1, 0, 0, 0, 0, 0, 0, 0, 0])],\n",
       " [array([0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
       "  array([0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       "  array([1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  array([0, 0, 0, 0, 0, 1, 0, 0, 0])],\n",
       " [array([0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       "  array([0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
       "  array([0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       "  array([0, 1, 0, 0, 0, 0, 0, 0, 0])],\n",
       " [array([0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
       "  array([1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  array([0, 1, 0, 0, 0, 0, 0, 0, 0])],\n",
       " [array([1, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 1, 0, 0, 0])],\n",
       " [array([0, 0, 0, 0, 0, 0, 0, 1, 0]), array([0, 0, 0, 0, 0, 0, 1, 0, 0])],\n",
       " [array([0, 0, 0, 0, 1, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 1, 0, 0])],\n",
       " [array([0, 0, 0, 0, 1, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 1, 0])]]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['heard'], dtype='<U5')]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt.inverse_transform(training_data[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array(['have'], dtype='<U5')],\n",
       " [array(['you'], dtype='<U5')],\n",
       " [array(['the'], dtype='<U5')],\n",
       " [array(['word'], dtype='<U5')]]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cnt.inverse_transform(i) for i in training_data[2][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'have': 2,\n",
       " 'you': 8,\n",
       " 'heard': 3,\n",
       " 'the': 5,\n",
       " 'word': 6,\n",
       " 'about': 0,\n",
       " 'bird': 1,\n",
       " 'hello': 4,\n",
       " 'world': 7}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = training_data[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 1, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 1]),\n",
       " array([0, 0, 0, 0, 1, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 1, 0])]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in yy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
