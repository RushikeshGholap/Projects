{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Loss for Implicit Feedback Neural Recommender Systems\n",
    "\n",
    "Goals:\n",
    "- Understand bi-linear recommendation system only using positive feedback data\n",
    "- Use Margin Based Comparator / Triplet Loss\n",
    "- Build deep learning architecture using similar deign principle\n",
    "\n",
    "This notebook is inspired by Oliver Grisel Notebook who used Keras\n",
    "https://github.com/ogrisel for building the moels. We will be using Basic Tensorflow APIs instead. You can also look into Maciej Kula's work [Recommendations in Keras using triplet loss](\n",
    "https://github.com/maciejkula/triplet_recommendations_keras) that uses BPR ( Bayesian Personalized Ranking ). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path as op\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.contrib import layers\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Path for MovieLens dataset\n",
    "ML_100K_PATH = os.path.join('processed','ml-100k','ml-100k')\n",
    "\n",
    "data_train = pd.read_csv(op.join(ML_100K_PATH, 'ua.base'), sep='\\t',\n",
    "                        names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "data_test = pd.read_csv(op.join(ML_100K_PATH, 'ua.test'), sep='\\t',\n",
    "                        names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "\n",
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_release_year(x):\n",
    "    splits = str(x).split('-')\n",
    "    if(len(splits) == 3):\n",
    "        return int(splits[2])\n",
    "    else:\n",
    "        return 1920\n",
    "    \n",
    "\n",
    "m_cols = ['item_id', 'title', 'release_date', 'video_release_date', 'imdb_url']\n",
    "items = pd.read_csv(op.join(ML_100K_PATH, 'u.item'), sep='|',\n",
    "                    names=m_cols, usecols=range(5), encoding='latin-1')\n",
    "items['release_year'] = items['release_date'].map(get_release_year)\n",
    "\n",
    "data_train = pd.merge(data_train, items)\n",
    "data_test = pd.merge(data_test, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_user_id = max(data_train['user_id'].max(), data_test['user_id'].max())\n",
    "max_item_id = max(data_train['item_id'].max(), data_test['item_id'].max())\n",
    "\n",
    "n_users = max_user_id + 1\n",
    "n_items = max_item_id + 1\n",
    "\n",
    "print('n_users=%d, n_items=%d' % (n_users, n_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit feedback data\n",
    "\n",
    "Consider ratings >= 4 as positive feed back and ignore the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['rating'].plot(kind='hist');\n",
    "print(data_train['rating'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data_train = data_train.query(\"rating >= 4\")\n",
    "pos_data_test = data_test.query(\"rating >= 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the mean rating is around 3.5, this cut will remove approximately half of the ratings from the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data_train['rating'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data_test['rating'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Triplet Loss\n",
    "\n",
    "The following section demonstrates how to build a low-rank quadratic interaction model between users and items. The similarity score between a user and an item is defined by the unormalized dot products of their respective embeddings.\n",
    "\n",
    "The matching scores can be use to rank items to recommend to a specific user.\n",
    "\n",
    "Training of the model parameters is achieved by randomly sampling negative items not seen by a pre-selected anchor user. We want the model embedding matrices to be such that the similarity between the user vector and the negative vector is smaller than the similarity between the user vector and the positive item vector. Furthermore we use a margin to further move appart the negative from the anchor user.\n",
    "\n",
    "Here is the architecture of such a triplet architecture. The triplet name comes from the fact that the loss to optimize is defined for triple `(anchor_user, positive_item, negative_item)`:\n",
    "\n",
    "<img src=\"images/rec_archi_implicit_2.svg\" style=\"width: 600px;\" />\n",
    "\n",
    "We call this model a triplet model with bi-linear interactions because the similarity between a user and an item is captured by a dot product of the first level embedding vectors. This is therefore not a deep architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the actual code that builds the model(s) with shared weights. Note that here we use the cosine similarity instead of unormalized dot products (both seems to yield comparable results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality of Ranked Recommendations\n",
    "\n",
    "Now that we have a randomly initialized model we can start computing random recommendations. To assess their quality we do the following for each user:\n",
    "\n",
    "- compute matching scores for items (except the movies that the user has already seen in the training set),\n",
    "- compare to the positive feedback actually collected on the test set using the ROC AUC ranking metric,\n",
    "- average ROC AUC scores across users to get the average performance of the recommender model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the model should make predictions that rank the items in random order. The **ROC AUC score** is a ranking score that represents the **expected value of correctly ordering uniformly sampled pairs of recommendations**.\n",
    "\n",
    "A random (untrained) model should yield 0.50 ROC AUC on average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 64 # embedding size\n",
    "reg_param = 0.01 # regularization parameter lambda\n",
    "learning_rate = 0.01 # learning rate \n",
    "margin = 1.0 # margin \n",
    "\n",
    "# create tensorflow graph\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    # setting up random seed\n",
    "    tf.set_random_seed(1234)\n",
    "    \n",
    "    # placeholders\n",
    "    user_input = tf.placeholder(shape=[None], dtype=tf.int64)\n",
    "    positive_item_input = tf.placeholder(shape=[None], dtype=tf.int64)\n",
    "    negative_item_input = tf.placeholder(shape=[None], dtype=tf.int64)\n",
    "    \n",
    "    # variables\n",
    "    with tf.variable_scope(\"embedding\"):\n",
    "        user_weight = tf.get_variable(\"user_w\"\n",
    "                                      , shape=[max_user_id + 1, embedding_size]\n",
    "                                      , dtype=tf.float32\n",
    "                                      , initializer=layers.xavier_initializer())\n",
    "\n",
    "        item_weight = tf.get_variable(\"item_w\"\n",
    "                                       , shape=[max_item_id + 1, embedding_size]\n",
    "                                       , dtype=tf.float32\n",
    "                                       , initializer=layers.xavier_initializer())\n",
    "    # embedding\n",
    "    with tf.name_scope(\"embedding\"):\n",
    "        user_embedding = tf.nn.embedding_lookup(user_weight, user_input)\n",
    "        positive_item_embedding = tf.nn.embedding_lookup(item_weight, positive_item_input)\n",
    "        negative_item_embedding = tf.nn.embedding_lookup(item_weight, negative_item_input)\n",
    "     \n",
    "     # similarity\n",
    "    with tf.name_scope(\"similarity\"):\n",
    "        positive_similarity = tf.reduce_sum(tf.multiply(user_embedding, positive_item_embedding), 1) \n",
    "        negative_similarity = tf.reduce_sum(tf.multiply(user_embedding, negative_item_embedding), 1) \n",
    "        \n",
    "    # loss \n",
    "    with tf.name_scope(\"loss\"):\n",
    "        triplet_loss = tf.maximum(negative_similarity - positive_similarity + margin, 0)\n",
    "        loss = tf.reduce_mean(triplet_loss)\n",
    "        train_ops = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_triplets(pos_data, max_item_id, random_seed=0):\n",
    "    \"\"\"Sample negatives at random\"\"\"\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    user_ids = pos_data['user_id'].values\n",
    "    pos_item_ids = pos_data['item_id'].values\n",
    "\n",
    "    neg_item_ids = rng.randint(low=1, high=max_item_id + 1,\n",
    "                               size=len(user_ids))\n",
    "    return [ user_ids, pos_item_ids,neg_item_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Triplet Model\n",
    "\n",
    "Let's now fit the parameters of the model by sampling triplets: for each user, select a movie in the positive feedback set of that user and randomly sample another movie to serve as negative item.\n",
    "\n",
    "Note that this sampling scheme could be improved by removing items that are marked as positive in the data to remove some label noise. In practice this does not seem to be a problem though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the triplet model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    # initializer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def check_overfit(validation_loss):\n",
    "        n = len(validation_loss)\n",
    "        if n < 5:\n",
    "            return False\n",
    "        count = 0 \n",
    "        for i in range(n-4, n):\n",
    "            if validation_loss[i] < validation_loss[i-1]:\n",
    "                count += 1\n",
    "            if count >=2:\n",
    "                return False\n",
    "        return True\n",
    "            \n",
    "    for i in range(n_epochs):\n",
    "        triplet_inputs_train = sample_triplets(pos_data_train, max_item_id,random_seed=i)\n",
    "        triplet_inputs_val = sample_triplets(pos_data_test, max_item_id,random_seed=i+1)\n",
    "        \n",
    "        train_input_dict = {user_input: triplet_inputs_train[0]\n",
    "                            , positive_item_input: triplet_inputs_train[1]\n",
    "                            , negative_item_input: triplet_inputs_train[2]}\n",
    "        \n",
    "        val_input_dict = {user_input: triplet_inputs_val[0]\n",
    "                            , positive_item_input: triplet_inputs_val[1]\n",
    "                            , negative_item_input: triplet_inputs_val[2]}\n",
    "        sess.run([train_ops], feed_dict=train_input_dict)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            loss_train = sess.run(loss, feed_dict=train_input_dict)\n",
    "            loss_val = sess.run(loss, feed_dict=val_input_dict)\n",
    "\n",
    "            losses_train.append(loss_train)\n",
    "            losses_val.append(loss_val)\n",
    "\n",
    "            # check early stopping \n",
    "            if(check_overfit(losses_val)):\n",
    "                print('overfit !')\n",
    "                break\n",
    "\n",
    "             \n",
    "            # calculate AUC Score \n",
    "            \"\"\"Compute the ROC AUC for each user and average over users\"\"\"\n",
    "            max_user_id = max(pos_data_train['user_id'].max(), pos_data_test['user_id'].max())\n",
    "            max_item_id = max(pos_data_train['item_id'].max(), pos_data_test['item_id'].max())\n",
    "            user_auc_scores = []\n",
    "            for user_id in range(1, max_user_id + 1):\n",
    "                pos_item_train = pos_data_train[pos_data_train['user_id'] == user_id]\n",
    "                pos_item_test = pos_data_test[pos_data_test['user_id'] == user_id]\n",
    "\n",
    "                # Consider all the items already seen in the training set\n",
    "                all_item_ids = np.arange(1, max_item_id + 1)\n",
    "                items_to_rank = np.setdiff1d(all_item_ids, pos_item_train['item_id'].values)\n",
    "\n",
    "                # Ground truth: return 1 for each item positively present in the test set\n",
    "                # and 0 otherwise.\n",
    "                expected = np.in1d(items_to_rank, pos_item_test['item_id'].values)\n",
    "\n",
    "                if np.sum(expected) >= 1:\n",
    "                    # At least one positive test value to rank\n",
    "                    repeated_user_id = np.empty_like(items_to_rank)\n",
    "                    repeated_user_id.fill(user_id)\n",
    "                    predicted = sess.run(positive_similarity, feed_dict={user_input : repeated_user_id, \n",
    "                                                            positive_item_input : items_to_rank})\n",
    "                    user_auc_scores.append(roc_auc_score(expected, predicted))\n",
    "\n",
    "            print(\"iteration : %d train loss: %.3f , valid loss %.3f , ROC auc %.4f\" % (i,loss_train, loss_val,np.mean(user_auc_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Deep Matching Model on Implicit Feedback\n",
    "\n",
    "\n",
    "Instead of using hard-coded cosine similarities to predict the match of a `(user_id, item_id)` pair, we can instead specify a deep neural network based parametrisation of the similarity. The parameters of that matching model are also trained with the margin comparator loss:\n",
    "\n",
    "<img src=\"images/rec_archi_implicit_1.svg\" style=\"width: 600px;\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embedding_size = 32\n",
    "item_embedding_size = 64\n",
    "num_hidden = 64\n",
    "\n",
    "reg_param = 0.01\n",
    "learning_rate = 0.01\n",
    "n_users = max_user_id + 1\n",
    "n_items = max_item_id + 1\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    # setting up random seed\n",
    "    tf.set_random_seed(1234)\n",
    "\n",
    "    user_input = tf.placeholder(shape=[None,1], dtype=tf.int64, name='user_input')\n",
    "    positive_item_input = tf.placeholder(shape=[None,1], dtype=tf.int64, name='positive_item_input')\n",
    "    negative_item_input = tf.placeholder(shape=[None,1], dtype=tf.int64, name='negative_item_input')\n",
    "    \n",
    "    l2_loss = tf.constant(0.0)\n",
    "    \n",
    "    # embeddding layer\n",
    "    with tf.variable_scope(\"embedding\"):\n",
    "        user_weights = tf.get_variable(\"user_w\"\n",
    "                                      , shape=[n_users, user_embedding_size]\n",
    "                                      , dtype=tf.float32\n",
    "                                      , initializer=layers.xavier_initializer())\n",
    "        \n",
    "        item_weights = tf.get_variable(\"item_w\"\n",
    "                                       , shape=[n_items, item_embedding_size]\n",
    "                                       , dtype=tf.float32\n",
    "                                       , initializer=layers.xavier_initializer())\n",
    "        \n",
    "        user_embedding = tf.squeeze(tf.nn.embedding_lookup(user_weights, user_input),axis=1, name='user_embedding')\n",
    "        positive_item_embedding = tf.squeeze(tf.nn.embedding_lookup(item_weights, positive_item_input),axis=1, name='positive_item_embedding')\n",
    "        negative_item_embedding = tf.squeeze(tf.nn.embedding_lookup(item_weights, negative_item_input),axis=1, name='negative_item_embedding')\n",
    "        \n",
    "        l2_loss += tf.nn.l2_loss(user_weights)\n",
    "        l2_loss += tf.nn.l2_loss(item_weights)\n",
    "        \n",
    "        \n",
    "        print(user_embedding)\n",
    "        print(positive_item_embedding)\n",
    "        print(negative_item_embedding)\n",
    "        \n",
    "    \n",
    "    # combine inputs\n",
    "    with tf.name_scope('concatenation'):\n",
    "        positive_embeddings_pair = tf.concat([user_embedding, positive_item_embedding], axis=1)\n",
    "        negative_embeddings_pair = tf.concat([user_embedding, negative_item_embedding], axis=1)\n",
    "        print(positive_embeddings_pair)\n",
    "        print(negative_embeddings_pair)\n",
    "        \n",
    "    # fc-1\n",
    "    \n",
    "    with tf.name_scope(\"fc_1\"):\n",
    "        W_fc_1 = tf.get_variable(\n",
    "            \"W_hidden\",\n",
    "            shape=[user_embedding_size + item_embedding_size, num_hidden],\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_fc_1 = tf.Variable(tf.constant(0.1, shape=[num_hidden]), name=\"b\")\n",
    "        hidden_output_positive = tf.nn.relu(tf.nn.xw_plus_b(positive_embeddings_pair, W_fc_1, b_fc_1), name='hidden_output_positive')\n",
    "        hidden_output_negative = tf.nn.relu(tf.nn.xw_plus_b(negative_embeddings_pair, W_fc_1, b_fc_1), name='hidden_output_negative')\n",
    "        \n",
    "        l2_loss += tf.nn.l2_loss(W_fc_1)\n",
    "        print(hidden_output_positive)\n",
    "        print(hidden_output_negative)\n",
    "        \n",
    "    # dropout\n",
    "    with tf.name_scope(\"dropout\"):\n",
    "        h_drop_positive = tf.nn.dropout(hidden_output_positive, 0.8, name=\"hidden_output_drop_positive\")\n",
    "        h_drop_negative = tf.nn.dropout(hidden_output_negative, 0.8, name=\"hidden_output_drop_negative\")\n",
    "        print(h_drop_positive)\n",
    "        print(h_drop_negative)\n",
    "    \n",
    "    # fc-2\n",
    "    with tf.name_scope(\"fc_2\"):\n",
    "        W_fc_2 = tf.get_variable(\n",
    "            \"W_output\",\n",
    "            shape=[num_hidden,1],\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b_fc_2 = tf.Variable(tf.constant(0.1, shape=[1]), name=\"b\")\n",
    "        positive_prediction = tf.nn.xw_plus_b(h_drop_positive, W_fc_2, b_fc_2, name='positive_prediction')\n",
    "        negative_prediction = tf.nn.xw_plus_b(h_drop_negative, W_fc_2, b_fc_2, name='negative_prediction')\n",
    "        \n",
    "        l2_loss += tf.nn.l2_loss(W_fc_2)\n",
    "        print(positive_prediction)\n",
    "        print(negative_prediction)\n",
    "\n",
    "    # loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        triplet_loss = tf.maximum(negative_prediction - positive_prediction + margin, 0)\n",
    "        loss = tf.reduce_mean(triplet_loss) + reg_param * l2_loss\n",
    "        train_ops = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # initializer\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def check_overfit(validation_loss):\n",
    "        n = len(validation_loss)\n",
    "        if n < 5:\n",
    "            return False\n",
    "        count = 0 \n",
    "        for i in range(n-4, n):\n",
    "            if validation_loss[i] < validation_loss[i-1]:\n",
    "                count += 1\n",
    "            if count >=3:\n",
    "                return False\n",
    "        return True\n",
    "            \n",
    "    for i in range(n_epochs):\n",
    "        triplet_inputs_train = sample_triplets(pos_data_train, max_item_id,random_seed=i)\n",
    "        triplet_inputs_val = sample_triplets(pos_data_test, max_item_id,random_seed=i+1)\n",
    "        \n",
    "        train_input_dict = {user_input: triplet_inputs_train[0].reshape([-1,1])\n",
    "                            , positive_item_input: triplet_inputs_train[1].reshape([-1,1])\n",
    "                            , negative_item_input: triplet_inputs_train[2].reshape([-1,1])}\n",
    "        \n",
    "        val_input_dict = {user_input: triplet_inputs_val[0].reshape([-1,1])\n",
    "                            , positive_item_input: triplet_inputs_val[1].reshape([-1,1])\n",
    "                            , negative_item_input: triplet_inputs_val[2].reshape([-1,1])}\n",
    "        sess.run([train_ops], feed_dict=train_input_dict)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            loss_train = sess.run(loss, feed_dict=train_input_dict)\n",
    "            loss_val = sess.run(loss, feed_dict=val_input_dict)\n",
    "\n",
    "            losses_train.append(loss_train)\n",
    "            losses_val.append(loss_val)\n",
    "\n",
    "            # check early stopping \n",
    "            if(check_overfit(losses_val)):\n",
    "                print('overfit !')\n",
    "                break\n",
    "\n",
    "            \n",
    "            # calculate AUC Score \n",
    "            \"\"\"Compute the ROC AUC for each user and average over users\"\"\"\n",
    "            max_user_id = max(pos_data_train['user_id'].max(), pos_data_test['user_id'].max())\n",
    "            max_item_id = max(pos_data_train['item_id'].max(), pos_data_test['item_id'].max())\n",
    "            user_auc_scores = []\n",
    "            for user_id in range(1, max_user_id + 1):\n",
    "                pos_item_train = pos_data_train[pos_data_train['user_id'] == user_id]\n",
    "                pos_item_test = pos_data_test[pos_data_test['user_id'] == user_id]\n",
    "\n",
    "                # Consider all the items already seen in the training set\n",
    "                all_item_ids = np.arange(1, max_item_id + 1)\n",
    "                items_to_rank = np.setdiff1d(all_item_ids, pos_item_train['item_id'].values)\n",
    "\n",
    "                # Ground truth: return 1 for each item positively present in the test set\n",
    "                # and 0 otherwise.\n",
    "                expected = np.in1d(items_to_rank, pos_item_test['item_id'].values)\n",
    "\n",
    "                if np.sum(expected) >= 1:\n",
    "                    # At least one positive test value to rank\n",
    "                    repeated_user_id = np.empty_like(items_to_rank)\n",
    "                    repeated_user_id.fill(user_id)\n",
    "                    predicted = sess.run(positive_prediction, feed_dict={user_input : repeated_user_id.reshape([-1,1]), \n",
    "                                                            positive_item_input : items_to_rank.reshape([-1,1])})\n",
    "                    user_auc_scores.append(roc_auc_score(expected, predicted))\n",
    "\n",
    "            print(\"iteration : %d train loss: %.3f , valid loss %.3f , ROC auc %.4f\" % (i,loss_train, loss_val,np.mean(user_auc_scores)))\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Extensions\n",
    "\n",
    "You can implement any of the following ideas if you want to get a deeper understanding of recommender systems.\n",
    "\n",
    "\n",
    "### Leverage User and Item metadata\n",
    "\n",
    "As we did for the Explicit Feedback model, it's also possible to extend our models to take additional user and item metadata as side information when computing the match score.\n",
    "\n",
    "\n",
    "### Better Ranking Metrics\n",
    "\n",
    "In this notebook we evaluated the quality of the ranked recommendations using the ROC AUC metric. This score reflect the ability of the model to correctly rank any pair of items (sampled uniformly at random among all possible items).\n",
    "\n",
    "In practice recommender systems will only display a few recommendations to the user (typically 1 to 10). It is typically more informative to use an evaluatio metric that characterize the quality of the top ranked items and attribute less or no importance to items that are not good recommendations for a specific users. Popular ranking metrics therefore include the **Precision at k** and the **Mean Average Precision**.\n",
    "\n",
    "\n",
    "\n",
    "### Hard Negatives Sampling\n",
    "\n",
    "In this experiment we sampled negative items uniformly at random. However, after training the model for a while, it is possible that the vast majority of sampled negatives have a similarity already much lower than the positive pair and that the margin comparator loss sets the majority of the gradients to zero effectively wasting a lot of computation.\n",
    "\n",
    "Given the current state of the recsys model we could sample harder negatives with a larger likelihood to train the model better closer to its decision boundary. This strategy is implemented in the WARP loss [1].\n",
    "\n",
    "The main drawback of hard negative sampling is increasing the risk of sever overfitting if a significant fraction of the labels are noisy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
